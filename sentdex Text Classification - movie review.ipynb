{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentdex Text Classification - movie review.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Y4c2BsvLndjL","colab_type":"text"},"source":["https://pythonprogramming.net/text-classification-nltk-tutorial/?completed=/wordnet-nltk-tutorial/\n","\n","The NLTK corpus movie_reviews data set has the reviews, and they are labeled already as positive or negative. This means we can train and test with this data\n","\n","Movie reviews database is part of the NLTK corpus. From there we'll try to use words as \"features\" which are a part of either a positive or negative movie review. "]},{"cell_type":"code","metadata":{"id":"ZW3IUAelm3Kp","colab_type":"code","colab":{}},"source":["import nltk\n","import random\n","#from nltk.corpus import movie_reviews\n","nltk.download('movie_reviews')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"czraBMjDoLwO","colab_type":"code","colab":{}},"source":["'''\n","In each category (we have pos or neg), take all of the file IDs (each review has its own ID), \n","then store the word_tokenized version (a list of words) for the file ID, followed by\n","the positive or negative label in one big list.\n","\n","documents = [(list(movie_reviews.words(fileid)), category)\n","             for category in movie_reviews.categories()\n","             for fileid in movie_reviews.fileids(category)]\n","             \n","random.shuffle(documents)\n","\n","print(documents[1])\n","\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9RDJ6hLqAjC","colab_type":"code","colab":{}},"source":["'''\n"," 'pos/cv902_12256.txt',\n"," 'pos/cv903_17822.txt',\n"," 'pos/cv904_24353.txt',......etc\n","'''\n","movie_reviews.fileids('pos')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPj_B85ApeBa","colab_type":"code","colab":{}},"source":["'''['the', 'trailers', 'and', 'the', 'beginning', 'of', ...]\n","'''\n","movie_reviews.words('pos/cv480_19817.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_a_sUmJJqdwz","colab_type":"code","colab":{}},"source":["# (list(movie_reviews.words('pos/cv480_19817.txt')), 'pos')\n","\n","len(list(movie_reviews.words('pos/cv480_19817.txt')))\n","\n","#(list(movie_reviews.words('pos/cv480_19817.txt')), 'pos')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8b3bkOZEobZ7","colab_type":"code","colab":{}},"source":["documents = []\n","\n","for category in movie_reviews.categories():\n","    #print(category)\n","    for fileid in movie_reviews.fileids(category):\n","        #print(fileid)\n","        documents = (list(movie_reviews.words(fileid)), category)\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHpPMpUSrPKK","colab_type":"code","colab":{}},"source":["all_words = []\n","for w in movie_reviews.words():\n","    all_words.append(w.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHD0gd5Zrpd6","colab_type":"code","colab":{}},"source":["len(all_words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cUR9oYCHttJU","colab_type":"text"},"source":["From here, we can perform a frequency distribution, to then find out the most common words.\n","\n","You can also find out how many occurences a word has by doing:"]},{"cell_type":"code","metadata":{"id":"TnOM8K5arfzh","colab_type":"code","colab":{}},"source":["all_words_freq = nltk.FreqDist(all_words)\n","print(all_words_freq.most_common(5))\n","print(all_words_freq[\"stupid\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTfho0RhuTk4","colab_type":"text"},"source":["## Next up, we'll begin storing our words as features of either positive or negative movie reviews.\n","\n","**Converting words to Features**"]},{"cell_type":"code","metadata":{"id":"ro4UDg_bvQ_U","colab_type":"code","colab":{}},"source":["all_words_freq.keys()\n","\n","all_words_freq\n","\n","list(all_words_freq.keys())[:5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbwIdvRwvzHA","colab_type":"code","colab":{}},"source":["word_features = list(all_words_freq.keys())[:3000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8alNIceMv9yG","colab_type":"text"},"source":["word_features, which contains the top 3,000 most common words. \n","\n","Next, we're going to build a quick function that will find these top 3,000 words in our positive and negative documents, marking their presence as either positive or negative:"]},{"cell_type":"code","metadata":{"id":"kP0TQe9YwR1v","colab_type":"code","colab":{}},"source":["def find_features(document):\n","    words = set(document)\n","    features = {}\n","    for w in word_features:\n","        features[w] = (w in words)\n","\n","    return features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_k9W9WY-x7A9","colab_type":"code","colab":{}},"source":["words = set(movie_reviews.words('neg/cv000_29416.txt'))\n","\n","#print(len(words))\n","print(words)\n","features = {}\n","for w in word_features:\n","        features[w] = (w in words)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBXiMwgj0H8t","colab_type":"text"},"source":["Next, we can print one feature set for one document / text file like:"]},{"cell_type":"code","metadata":{"id":"vON7BtQsxkRf","colab_type":"code","colab":{}},"source":["print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XPUWMQst0BzL","colab_type":"text"},"source":["Then we can do this for all of our documents, saving the feature existence booleans and their respective positive or negative categories by doing:"]},{"cell_type":"code","metadata":{"id":"Po0AwHsL0VZt","colab_type":"code","colab":{}},"source":["featuresets = [(find_features(rev), category) for (rev, category) in documents]\n","\n","'''\n","for (rev, category) in documents:\n","    featuresets = [(find_features(rev), category)]\n","'''\n","\n","    \n","# output - {'plot': True, ':': True, 'two': True, 'teen': True, 'couples': True,......}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wyeibHOi1P_9","colab_type":"text"},"source":["Now that we have our features and labels, the next step is to go ahead and train an algorithm, then test it. \n","\n","So, let's go ahead and do that, starting with the Naive Bayes classifier in the next tutorial!"]},{"cell_type":"markdown","metadata":{"id":"dP9cBgN914mT","colab_type":"text"},"source":["**Training And Testing**\n","\n","We train by telling it \"this data is positive,\" or \"this data is negative.\" \n","\n","**Splitting Data:**\n"]},{"cell_type":"code","metadata":{"id":"XVmev5Be2ZNK","colab_type":"code","colab":{}},"source":["# set that we'll train our classifier with\n","training_set = featuresets[:1900]\n","\n","# set that we'll test against.\n","testing_set = featuresets[1900:]\n","\n","#now it is trained. Next, we can test it:\n","print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n","\n","classifier.show_most_informative_features(15)"],"execution_count":0,"outputs":[]}]}